---
title: Multi-level Modeling, Pairwise Comparisons and Effect Statistics in R
author: Ryan Curtis, PhD
date: '2020-08-14'
slug: multilevel-modeling-pairwise-comparisons-and-effect-statistics-for-sports-scientist
categories: []
tags: []
subtitle: ''
summary: ''
authors: []
lastmod: '2020-08-14T16:28:18-05:00'
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
---

<script src="/rmarkdown-libs/clipboard/clipboard.min.js"></script>
<link href="/rmarkdown-libs/primer-tooltips/build.css" rel="stylesheet" />
<link href="/rmarkdown-libs/klippy/css/klippy.min.css" rel="stylesheet" />
<script src="/rmarkdown-libs/klippy/js/klippy.min.js"></script>


<div id="why-should-sports-scientist-be-familiar-with-mixed-effects-multi-level-modeling-and-effect-statistics" class="section level3">
<h3>Why should sports scientist be familiar with mixed effects (multi-level) modeling and effect statistics…</h3>
<p>Skill in analyzing longitudinal data is important for a number of practical reasons including; accounting for the dependencies created by repeated measures (athletes being measured over time), dealing with missing or unbalanced data (common occurrence in athlete monitoring practices), differentiating between-athlete from within-athlete variability, accounting for time-varying (e.g., workload, perceived fatigue) and time-invariant factors (e.g., gender, role, position) and establishing the influence of time <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6169745/">(Windt et al. 2018)</a>. Multi-level models are very useful to a sports scientist because they can handle aforementioned complexities that are a natural part of player tracking/monitoring.</p>
<p>Measuring the magnitude or strength of the change of a health or performance outcome can also aid in the evaluation of the positive or negative consequences of an intervention or can be used to compare discrete groups/phases/conditions (e.g., male vs female, preseason vs. inseason, turf vs. grass surface). When assessing differences between said conditions, a common approach is using Cohen’s d, which is very similar to a standard difference score or z-score. Classic Cohen’s d requires the mean difference between conditions in the numerator and pooled standard deviation (i.e., combined standard deviation of each condition) in the denominator.</p>
<p>Cohen’s d Effect Size = (mean 1 – mean 2) ÷ between-athlete pooled SD</p>
<p>As mentioned, mixed models are unique becuase they can account for a number of variances (between-athlete/within-athlete, variability associated with time). However, this does present some issues when calculating classic cohen’s d directly from the model because Cohen’s d was developed for between-subjects research. With that in mind, the aim of this post is to set up a basic mixed model with player tracking (workload) data in R and explore the number of options one has to calculate effect statistics from the results.</p>
<script>
  addClassKlippyTo("pre.r, pre.markdown");
  addKlippy('left', 'top', 'auto', '1', 'Copy code', 'Copied!');
</script>
</div>
<div id="simulate-training-load-data" class="section level1">
<h1>Simulate Training Load Data</h1>
<p>First things first, we need some player load data…which is not freely available but we can simulate a dataset to work with. With this data we are creating 20 athletes, with 50 sessions each (1000 total observations), assigning a mix of sessions by season phase (10 preseason, 30 inseason, 10 postseason), assigning each athlete a position, and randomly assigning a Total Distance value from a normal distribution with the mean equal to 5000 meters and a standard deviation of 1000 meters.</p>
<p>The problem with randomly assigning player distances from a normal distribution is that there will not be random variance in load by player, thus defeating the purpose of using mixed effects modeling. The random effect would be 0…so we need to introduce some variability. To do this, we can create an effect coefficient for each athlete and apply that to each individual player’s session load.</p>
<p>Also, we are going to look at differences between season phases so we need to introduce some variability in our independent variable. For the purposes of this example, we are going to assume that distances are ~1.2x higher on average during the preseason as compared to inseason or postseason phases.</p>
<pre class="r"><code>library(tidyverse)
#set.seed for reproducibility
set.seed(415)

#simulate data
Athlete &lt;- rep(paste(&#39;Athlete&#39;, 1:20), each = 50)
Session &lt;- rep(1:50, times = 20)
Season &lt;- rep(c(&quot;Preseason&quot;, &quot;Inseason&quot;, &quot;Postseason&quot;), times = c(10, 30, 10))
Position &lt;- rep(c(&quot;Point Guard&quot;, &quot;Shooting Guard&quot;, &quot;Small Forward&quot;, &quot;Power Forward&quot;, &quot;Center&quot;), each = 50)
Distance &lt;- rnorm(n = length(Athlete), mean = 5000, sd = 1000)


#create athlete effects
athleteeff = rnorm(20, 1, 0.1) 
athleteeff_data &lt;- data.frame(Athlete = paste(&#39;Athlete&#39;, 1:20), athleteeff)


#merge simulated data into a dataframe
data &lt;- data.frame(Athlete, Session, Season, Position, Distance) %&gt;%
  #increase preseason loads by 1.2x
  mutate(Distance = case_when(
    Season == &quot;Preseason&quot; ~ Distance*1.2, 
    TRUE ~ Distance
  )
) %&gt;%
  #join player effect coefficient
  left_join(athleteeff_data, &quot;Athlete&quot;) %&gt;%
  #multiply distances by each athletes effect coefficient
  mutate(Distance = Distance*athleteeff) %&gt;%
  select(-athleteeff)</code></pre>
<p>It’s always good to take a look at the structure of the data. Main thing is we want to make sure the data is in long format and each column is formatted appropriately (numeric, date, factor, etc.).</p>
<pre class="r"><code>#structure of dataframe
str(data)</code></pre>
<pre><code>## &#39;data.frame&#39;:    1000 obs. of  5 variables:
##  $ Athlete : Factor w/ 20 levels &quot;Athlete 1&quot;,&quot;Athlete 10&quot;,..: 1 1 1 1 1 1 1 1 1 1 ...
##  $ Session : int  1 2 3 4 5 6 7 8 9 10 ...
##  $ Season  : Factor w/ 3 levels &quot;Inseason&quot;,&quot;Postseason&quot;,..: 3 3 3 3 3 3 3 3 3 3 ...
##  $ Position: Factor w/ 5 levels &quot;Center&quot;,&quot;Point Guard&quot;,..: 2 2 2 2 2 2 2 2 2 2 ...
##  $ Distance: num  6438 5325 10839 6508 8265 ...</code></pre>
<pre class="r"><code>#view dataframe
head(data)</code></pre>
<pre><code>##     Athlete Session    Season    Position  Distance
## 1 Athlete 1       1 Preseason Point Guard  6438.306
## 2 Athlete 1       2 Preseason Point Guard  5325.113
## 3 Athlete 1       3 Preseason Point Guard 10839.473
## 4 Athlete 1       4 Preseason Point Guard  6507.579
## 5 Athlete 1       5 Preseason Point Guard  8265.098
## 6 Athlete 1       6 Preseason Point Guard  6645.504</code></pre>
</div>
<div id="fit-mixed-effects-model-with-lme4" class="section level1">
<h1>Fit Mixed Effects Model with lme4</h1>
<p>With this model, we are doing a basic LMM, with Distance covered as our dependent variable, Season (preseason, inseason and postseason) as our independent variable (fixed effect) and we are clustering the data by athlete (random effect). Although one of the benefits of LMM is its abililtiy to incorporate multiple factors into a model (providing the ability to account for the variance of various contextual factors), for these purposes we’ll keep the model simple.</p>
<p>I always load the <strong>lmerTest</strong> package instead of <strong>lme4</strong> by itself, doing so will load <strong>lme4</strong> by default and the output will give you a p-value for each coefficient (which is typically needed in academia…not so much elsewhere).</p>
<pre class="r"><code>library(lmerTest)

#fit model
fit &lt;- lmer(Distance ~ Season + (1|Athlete), data = data)

#view results
summary(fit)</code></pre>
<pre><code>## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [
## lmerModLmerTest]
## Formula: Distance ~ Season + (1 | Athlete)
##    Data: data
## 
## REML criterion at convergence: 16867.6
## 
## Scaled residuals: 
##     Min      1Q  Median      3Q     Max 
## -3.4902 -0.5926 -0.0112  0.6361  4.1010 
## 
## Random effects:
##  Groups   Name        Variance Std.Dev.
##  Athlete  (Intercept)  336657   580.2  
##  Residual             1217128  1103.2  
## Number of obs: 1000, groups:  Athlete, 20
## 
## Fixed effects:
##                  Estimate Std. Error      df t value Pr(&gt;|t|)    
## (Intercept)       5103.63     137.34   20.75  37.161   &lt;2e-16 ***
## SeasonPostseason    36.71      90.08  978.00   0.408    0.684    
## SeasonPreseason   1166.00      90.08  978.00  12.944   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Correlation of Fixed Effects:
##             (Intr) SsnPst
## SeasnPstssn -0.164       
## SeasonPrssn -0.164  0.250</code></pre>
<p>We are given a few important metrics from the <mark>summary()</mark> function, such as the model estimates (coefficients) for each factor and also significant differences (to be explained). Fixed effects factor coefficients are listed in alphabetical order by default, so the Inseason phase (i.e. intercept) is listed first. If you want a particular factor as the intercept then you need to create an Ordered Factor (you can do this with the <mark>fct_relevel()</mark> function, which is available when you load the <strong>tidyverse</strong> package).</p>
<p>Even though we are working with a categorical independent variable it’s helpful to keep in mind that these models are linear regressions (lmer = Linear Mixed Effects Regression), so the first factor will always be the intercept. The significance of the first variable is always telling you if the intercept is significantly different from 0…it’s not telling you anything about its relationship with the other factors. Important things with this model summary are that the preseason is significantly different from inseason (at this point we don’t know if preseason loads are different from postseason loads). If we want to know where the differences are between factors, we need to run a post-hoc/follow-up statisical test such as Tukey HSD (which we will).</p>
</div>
<div id="a-more-detailed-summary" class="section level1">
<h1>A More Detailed Summary…</h1>
<p>I prefer to use the <mark>tab_model()</mark> function in the sjPlot package by Daniel Lüdecke to get a more detailed view of the model. You get a nice html table output that is publication worthy. There are several options to customize the table output, you can find that tutorial <a href="https://strengejacke.github.io/sjPlot/articles/tab_model_estimates.html">here</a>. In addition to the standard components you get with a traditional <mark>summary()</mark> output, <mark>tab_model()</mark> provides a few other components we need to interpet the model such as the ICC value and marginal/conditional R<sup>2</sup> values.</p>
<p>The higher the ICC value the more justified the use of LMM vs. a more traditional statistic (repeated measures ANOVA would be comparable in this instance). ICC’s can be interpeted as the proportion of variance explained by clustering. An ICC &gt; 0.1 is generally accepted as the minimal threshold for justifying the use of LMM, so in other words at least 10% of the variance in Distance should be explained by clustering the observations by player.</p>
<pre class="r"><code># detailed summary
library(sjPlot)
tab_model(fit)</code></pre>
<table style="border-collapse:collapse; border:none;">
<tr>
<th style="border-top: double; text-align:center; font-style:normal; font-weight:bold; padding:0.2cm;  text-align:left; ">
 
</th>
<th colspan="3" style="border-top: double; text-align:center; font-style:normal; font-weight:bold; padding:0.2cm; ">
Distance
</th>
</tr>
<tr>
<td style=" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  text-align:left; ">
Predictors
</td>
<td style=" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  ">
Estimates
</td>
<td style=" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  ">
CI
</td>
<td style=" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  ">
p
</td>
</tr>
<tr>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; ">
(Intercept)
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  ">
5103.63
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  ">
4834.45 – 5372.80
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  ">
<strong>&lt;0.001
</td>
</tr>
<tr>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; ">
Season [Postseason]
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  ">
36.71
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  ">
-139.84 – 213.26
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  ">
0.684
</td>
</tr>
<tr>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; ">
Season [Preseason]
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  ">
1166.00
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  ">
989.45 – 1342.55
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  ">
<strong>&lt;0.001
</td>
</tr>
<tr>
<td colspan="4" style="font-weight:bold; text-align:left; padding-top:.8em;">
Random Effects
</td>
</tr>
<tr>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; padding-top:0.1cm; padding-bottom:0.1cm;">
σ<sup>2</sup>
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left;" colspan="3">
1217128.28
</td>
<tr>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; padding-top:0.1cm; padding-bottom:0.1cm;">
τ<sub>00</sub> <sub>Athlete</sub>
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left;" colspan="3">
336657.17
</td>
<tr>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; padding-top:0.1cm; padding-bottom:0.1cm;">
ICC
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left;" colspan="3">
0.22
</td>
<tr>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; padding-top:0.1cm; padding-bottom:0.1cm;">
N <sub>Athlete</sub>
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left;" colspan="3">
20
</td>
<tr>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; padding-top:0.1cm; padding-bottom:0.1cm; border-top:1px solid;">
Observations
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left; border-top:1px solid;" colspan="3">
1000
</td>
</tr>
<tr>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; padding-top:0.1cm; padding-bottom:0.1cm;">
Marginal R<sup>2</sup> / Conditional R<sup>2</sup>
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left;" colspan="3">
0.121 / 0.312
</td>
</tr>
</table>
<p>In this instance, the ICC is 0.22 so the use of LMM is appropriate.</p>
<p>The other outputs of interest are our Marginal and Conditional R<sup>2</sup> values (calculated based on <a href="https://besjournals.onlinelibrary.wiley.com/doi/full/10.1111/j.2041-210x.2012.00261.x">Nakagawa et al.</a>). These show the proportion of variance explained by the fixed effect only (Marginal R<sup>2</sup>) and fixed + random effect (Conditional R<sup>2</sup>), respectively.</p>
<p>So for this model, we would conclude that season phase is explaining ~12% of the distance covered in each session. It’s important to have this context becuase although there are significant differences in distance covered between the season phases, season phase by itself is not a strong determinant of session distance. On the other hand, the conditional R<sup>2</sup> is ~0.31, so we can see that the proportion of variance explained by the combination of season phase and accounting for individual variance is much higher. When we sum the marginal R<sup>2</sup> and the ICC value, we get a value quite close to our Conditional R<sup>2</sup>, this makes sense.</p>
</div>
<div id="visualize-model-effects" class="section level1">
<h1>Visualize Model Effects</h1>
<p>If you want to visualize the model (or any other mixed effects model [generalized, ordinal, etc.]), I would recommend using the ggeffects package (also by Daniel Lüdecke) combined with ggplot2 functions. You can find the ggeffects tutorial <a href="https://strengejacke.github.io/ggeffects/">here</a>. You can use the <strong>fit</strong> object with <mark>ggpredict()</mark> to create a data frame for plotting, pass a <mark>plot()</mark> function, and then add <mark>ggplot2()</mark> functions as needed to clean up the visualization. Here is a sample output from <mark>ggpredict()</mark>.</p>
<pre class="r"><code>library(ggeffects)
library(ggplot2)

#create plot dataframe
plot_data &lt;- ggpredict(fit, terms = c(&quot;Season&quot;))
plot_data</code></pre>
<pre><code>## 
## # Predicted values of Distance
## # x = Season
## 
## x          | Predicted |     SE |             95% CI
## ----------------------------------------------------
## Inseason   |   5103.63 | 137.34 | [4834.45, 5372.80]
## Postseason |   5140.34 | 151.39 | [4843.62, 5437.05]
## Preseason  |   6269.63 | 151.39 | [5972.91, 6566.34]
## 
## Adjusted for:
## * Athlete = 0 (population-level)</code></pre>
<p>Here is the plot output…</p>
<pre class="r"><code>#create plot
plot_data %&gt;%
  #reorder factor levels for plotting
  mutate(x = ordered(x, levels = c(&quot;Preseason&quot;, &quot;Inseason&quot;, &quot;Postseason&quot;))) %&gt;%
  #use plot function with ggpredict objects
  plot() + 
  #add ggplot2 as needed
  theme_blank() + ylim(c(3000,7000)) + ggtitle(&quot;Session Distance by Season Phase&quot;)</code></pre>
<p><img src="/post/2020-08-14-multilevel-modelling-pairwise-comparisons-and-effect-statistics-for-sports-scientist.en_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
</div>
<div id="fit-tukey-for-pairwise-comparisons" class="section level1">
<h1>Fit Tukey for Pairwise Comparisons</h1>
<p>Since the model summary indicates we have some significant differences in session distance by season phase, the next step is to compare the levels against each other to see where the differences lie. We can tell by the model <mark>summary()</mark> output that Inseason and Postseason are significantly different than Preseason (model intercept), but we want to run this model through a post hoc test to confirm where the differences are.</p>
<pre class="r"><code>library(multcomp)

# pairwise comparisons
fit_tukey &lt;- glht(fit, linfct=mcp(Season=&quot;Tukey&quot;))
summary(fit_tukey)</code></pre>
<pre><code>## 
##   Simultaneous Tests for General Linear Hypotheses
## 
## Multiple Comparisons of Means: Tukey Contrasts
## 
## 
## Fit: lmer(formula = Distance ~ Season + (1 | Athlete), data = data)
## 
## Linear Hypotheses:
##                             Estimate Std. Error z value Pr(&gt;|z|)    
## Postseason - Inseason == 0     36.71      90.08   0.408    0.911    
## Preseason - Inseason == 0    1166.00      90.08  12.944   &lt;1e-05 ***
## Preseason - Postseason == 0  1129.29     110.32  10.236   &lt;1e-05 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## (Adjusted p values reported -- single-step method)</code></pre>
<p>As expected based on the model summary, both inseason and postseason session loads are less than the preseason. This makes sense. Preseason is used as a prepatory period and is accompanied by increased player loading.</p>
<p>Another important thing to note here, the difference estimates from this Tukey are the same as the lmer model summary, so we can also use these as our mean differences for the effect size calculations.</p>
</div>
<div id="calculating-effect-sizes" class="section level1">
<h1>Calculating Effect Sizes</h1>
<p>I’m not aware of any consensus on the best or preferred method of calculating effect sizes from within-subject designs. Jake Westfall goes into great detail <a href="http://jakewestfall.org/blog/index.php/2016/03/25/five-different-cohens-d-statistics-for-within-subject-designs/">here</a> on the subject, detailing several of the ways cohens d and d-like effect sizes can be estimated. The important thing to note here is transparency when reporting a standardized effect size.</p>
<div id="option-1-classical-cohens-d-calculation" class="section level2">
<h2>Option 1: Classical Cohen’s D Calculation</h2>
<p>The <mark>effectsize()</mark> function in the <a href="https://cran.r-project.org/web/packages/effectsize/effectsize.pdf">effectsize</a> package will give you classic Cohen’s d effect statistics from the lmer model output…</p>
<pre class="r"><code>library(effectsize)
 
effectsize::effectsize(fit) </code></pre>
<pre><code>## Parameter        | Coefficient (std.) |        95% CI
## -----------------------------------------------------
## (Intercept)      |              -0.18 | [-0.39, 0.02]
## SeasonPostseason |               0.03 | [-0.11, 0.16]
## SeasonPreseason  |               0.88 | [ 0.75, 1.02]</code></pre>
<p>However, this output is somewhat limited, in that we are missing the Preseason - Postseason comparison effect statistic. The <strong>effectsize</strong> package supports lmerTest objects but unfortunatly will not work with our Tukey output. To get all comparison effect statistics using a traditional Cohen’s d, we can directly compare each variable.</p>
<p>To do that, we need to separate out the players distances by season phase.</p>
<pre class="r"><code>preseason_data &lt;- data %&gt;% filter(Season == &quot;Preseason&quot;) %&gt;% dplyr::select(Distance)
inseason_data &lt;- data %&gt;% filter(Season == &quot;Inseason&quot;) %&gt;% dplyr::select(Distance)
postseason_data &lt;- data %&gt;% filter(Season == &quot;Postseason&quot;) %&gt;% dplyr::select(Distance)</code></pre>
<p>There are several ways to do this. Firstly, you can write a cohen’s d function yourself. Credit to <a href="https://stackoverflow.com/questions/15436702/estimate-cohens-d-for-effect-size">this</a> post on StackOverflow. Note here that I’m not using the absolute value of the mean difference - as in the original cohen equation - this way we can see the direction of the effect.</p>
<pre class="r"><code>cohens_d &lt;- function(x, y) {
  lx &lt;- length(x)- 1                  # Sample Size X
  ly &lt;- length(y)- 1                  # Sample Size Y
  md  &lt;- mean(x) - mean(y)            # Mean Difference
  csd &lt;- lx * var(x) + ly * var(y)
  csd &lt;- csd/(lx + ly)
  csd &lt;- sqrt(csd)                    # Pooled SD
  
  md/csd                              # Cohen&#39;s d
}


cohens_d(inseason_data$Distance, preseason_data$Distance)</code></pre>
<pre><code>## [1] -0.9157833</code></pre>
<p>…or there are R packages for computing classical cohen’s d such as <a href="https://cran.r-project.org/web/packages/lsr/lsr.pdf">lsr</a> and <a href="https://cran.r-project.org/web/packages/effsize/effsize.pdf">effsize</a>…please note that these functions take the absolute value of the mean difference so all results will be positive.</p>
<pre class="r"><code>#install.packages(&quot;lsr&quot;)
#install.packages(&quot;effsize&quot;)
library(lsr)
lsr::cohensD(preseason_data$Distance, inseason_data$Distance)</code></pre>
<pre><code>## [1] 0.9157833</code></pre>
<p>The <strong>effsize</strong> package has a cohen’s d function which will also give you 95%CI.</p>
<pre class="r"><code>library(effsize)
effsize::cohen.d(preseason_data$Distance, inseason_data$Distance)</code></pre>
<pre><code>## 
## Cohen&#39;s d
## 
## d estimate: 0.9157833 (large)
## 95 percent confidence interval:
##     lower     upper 
## 0.7493283 1.0822383</code></pre>
<p>Given the options, let’s go ahead and use the <mark>cohen_d()</mark> function we wrote, create a dataframe and knit it into a table.</p>
<pre class="r"><code>#gather cohen effect sizes
preseason_inseason_cohen &lt;- cohens_d(preseason_data$Distance, inseason_data$Distance)
preseason_postseason_cohen &lt;- cohens_d(preseason_data$Distance, postseason_data$Distance)
postseason_inseason_cohen &lt;- cohens_d(postseason_data$Distance, inseason_data$Distance)


#make cohen dataframe
effect_data_cohen &lt;- data.frame(
  &quot;Season&quot; = c(&quot;Preseason - Inseason&quot;, &quot;Preseason - Postseason&quot;, &quot;Postseason - Inseason&quot;),
  &quot;Effect Size Cohen&quot; = c(preseason_inseason_cohen, preseason_postseason_cohen, postseason_inseason_cohen))%&gt;%
  rename(`Effect Size (Cohen)` = Effect.Size.Cohen) %&gt;%
  mutate(`Effect Size (Cohen)` = round(`Effect Size (Cohen)`, 2))
  

knitr::kable(effect_data_cohen)</code></pre>
<table>
<thead>
<tr class="header">
<th align="left">Season</th>
<th align="right">Effect Size (Cohen)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Preseason - Inseason</td>
<td align="right">0.92</td>
</tr>
<tr class="even">
<td align="left">Preseason - Postseason</td>
<td align="right">0.88</td>
</tr>
<tr class="odd">
<td align="left">Postseason - Inseason</td>
<td align="right">0.03</td>
</tr>
</tbody>
</table>
</div>
<div id="option-2-standardized-effect-size-using-residual-standard-deviation" class="section level2">
<h2>Option 2: Standardized Effect Size using Residual Standard Deviation</h2>
<p>This option is somewhat popular because the residual standard deviation is often assumed equivalent to the between-subjects standard deviation. However, that’s not always the case and is highly dependent on the data.</p>
<div id="extract-mean-differences" class="section level3">
<h3>Extract Mean Differences</h3>
<p>For the numerator we need mean differences, which we can grab from the Tukey summary (or you could gather them from the lmer model summary, results are the same…your choice).</p>
<pre class="r"><code>#make tukey summary into an object
res &lt;- summary(fit_tukey)

#extract first tukey comparison and make into an object
meandiff_1 &lt;- res$test[-(1:2)]$coefficients[1]


#extract second tukey comparison and make into an object
meandiff_2 &lt;- res$test[-(1:2)]$coefficients[2]


#extract third tukey comparison and make into an object
meandiff_3 &lt;- res$test[-(1:2)]$coefficients[3]</code></pre>
<p>One thing to consider when doing the extraction this way is that we are working Named Numbers, which you can see below when running the first coefficient.</p>
<pre class="r"><code>res$test[-(1:2)]$coefficients[1]</code></pre>
<pre><code>## Postseason - Inseason 
##              36.70945</code></pre>
<p>If you want just the number, then you can use the <mark>unname()</mark> function.</p>
<pre class="r"><code>unname(meandiff_1)</code></pre>
<pre><code>## [1] 36.70945</code></pre>
<p>If you want the name of the object, use <mark>names()</mark>.</p>
<pre class="r"><code>names(meandiff_1)</code></pre>
<pre><code>## [1] &quot;Postseason - Inseason&quot;</code></pre>
<p>We’ve already extracted mean differences for each comparison, so we only need to gather the residual standard deviation for this one. There are a few ways to do this, but the easiest is to call <mark>sigma</mark> from the model summary.</p>
<pre class="r"><code>#extract the sd directly
summary(fit)$sigma</code></pre>
<pre><code>## [1] 1103.235</code></pre>
<pre class="r"><code>#create pooled residual sd object
pooled_sd_residual &lt;- summary(fit)$sigma</code></pre>
<pre class="r"><code>#calculate effect sizes
effect_1_residual &lt;- meandiff_1/pooled_sd_residual
effect_2_residual &lt;- meandiff_2/pooled_sd_residual
effect_3_residual &lt;- meandiff_3/pooled_sd_residual




#season column
Season &lt;- c(names(meandiff_1), names(meandiff_2), names(meandiff_3))
#effects column
`Effect Size (Residual)` &lt;- c(effect_1_residual, effect_2_residual, effect_3_residual)

#make residual effect dataframe
effect_data_residual &lt;- as.data.frame(Season, `Effect Size (Residual)`) %&gt;%
    mutate(`Effect Size (Residual)` = round(`Effect Size (Residual)`, 2))


#knit into table
knitr::kable(
  effect_data_residual
)</code></pre>
<table>
<thead>
<tr class="header">
<th align="left">Season</th>
<th align="right">Effect Size (Residual)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Postseason - Inseason</td>
<td align="right">0.03</td>
</tr>
<tr class="even">
<td align="left">Preseason - Inseason</td>
<td align="right">1.06</td>
</tr>
<tr class="odd">
<td align="left">Preseason - Postseason</td>
<td align="right">1.02</td>
</tr>
</tbody>
</table>
</div>
</div>
<div id="option-3-standardized-effect-size-using-all-variance-components" class="section level2">
<h2>Option 3: Standardized Effect Size using All Variance Components</h2>
<p>Probably the effect size calculation with the most academic support is taking the square root of the sum of all variance components in the model. This method was first discussed by Westfall, Judd, and Kenny <a href="https://doi.apa.org/doiLanding?doi=10.1037%2Fxge0000014">(2014)</a> and later included in a statisical tutorial of Effect Sizes and Power Analysis in Mixed Effects Models by Marc Brysbaert and Michaël Stevens <a href="https://www.journalofcognition.org/articles/10.5334/joc.10/">(2018)</a>.</p>
<p>For this one we obviously need to gather all random effect variance components, which in this model are both the residual and within-athlete variances. You’ve already seen these in the model summary, so these calculations can be done simply by hand, but since we’re using R lets extract them using <mark>VarCorr()</mark>.</p>
<pre class="r"><code>#make VarCorr object from the model fit
vc &lt;- VarCorr(fit)

#square root of the sum of model variance components (pooled standard deviation)
sqrt(as.data.frame(vc)$vcov[1] + as.data.frame(vc)$vcov[2])</code></pre>
<pre><code>## [1] 1246.509</code></pre>
<pre class="r"><code>#make pooled sd object
pooled_sd_allvar &lt;- sqrt(as.data.frame(vc)$vcov[1] + as.data.frame(vc)$vcov[2])</code></pre>
<pre class="r"><code>#calculate effect sizes
effect_1_allvar &lt;- meandiff_1/pooled_sd_allvar
effect_2_allvar &lt;- meandiff_2/pooled_sd_allvar
effect_3_allvar &lt;- meandiff_3/pooled_sd_allvar


#make All Variance effect column
`Effect Size (All Variance)` &lt;- c(effect_1_allvar, effect_2_allvar, effect_3_allvar)


#make all variance dataframe
effect_data_allvar &lt;-   as.data.frame(Season, `Effect Size (All Variance)`) %&gt;%
    mutate(`Effect Size (All Variance)` = round(`Effect Size (All Variance)`, 2))


#knit into table
knitr::kable(
effect_data_allvar 
)</code></pre>
<table>
<thead>
<tr class="header">
<th align="left">Season</th>
<th align="right">Effect Size (All Variance)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Postseason - Inseason</td>
<td align="right">0.03</td>
</tr>
<tr class="even">
<td align="left">Preseason - Inseason</td>
<td align="right">0.94</td>
</tr>
<tr class="odd">
<td align="left">Preseason - Postseason</td>
<td align="right">0.91</td>
</tr>
</tbody>
</table>
<p>For comparison purposes, we can combine all three effect statistics into a single dataframe.</p>
<pre class="r"><code>knitr::kable(
effect_data_cohen %&gt;%
  left_join(effect_data_residual, by = &quot;Season&quot;) %&gt;%
  left_join(effect_data_allvar, by = &quot;Season&quot;)
)</code></pre>
<table>
<colgroup>
<col width="24%" />
<col width="21%" />
<col width="24%" />
<col width="29%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">Season</th>
<th align="right">Effect Size (Cohen)</th>
<th align="right">Effect Size (Residual)</th>
<th align="right">Effect Size (All Variance)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Preseason - Inseason</td>
<td align="right">0.92</td>
<td align="right">1.06</td>
<td align="right">0.94</td>
</tr>
<tr class="even">
<td align="left">Preseason - Postseason</td>
<td align="right">0.88</td>
<td align="right">1.02</td>
<td align="right">0.91</td>
</tr>
<tr class="odd">
<td align="left">Postseason - Inseason</td>
<td align="right">0.03</td>
<td align="right">0.03</td>
<td align="right">0.03</td>
</tr>
</tbody>
</table>
<div id="add-effect-size-interpretations" class="section level3">
<h3>Add Effect Size Interpretations</h3>
<p>Lastly, we can add interpretations to these effects. There have a been different propositions regarding the qualitative interpretation of ES’s. The original interpretation given by Cohen suggests &lt;0.5 is a small effect, 0.5 to 0.7 is a moderate effect, and &gt;0.8 is interpreted as a large effect. More recently Hopkins suggested a revised scale with threshold values at 0.2 (small), 0.6 (moderate), 1.2 (large), 2.0 (very large) and &gt;4 (nearly perfect).</p>
<pre class="r"><code>effect_interpretation &lt;- effect_data_cohen %&gt;%
   #interpret effect size
  mutate(`Hopkins Interpretation` = ifelse(abs(`Effect Size (Cohen)`) &lt;= 0.2, &quot;Trivial&quot;, ifelse(abs(`Effect Size (Cohen)`) &lt;= 0.6, &quot;Small&quot;, ifelse(abs(`Effect Size (Cohen)`) &lt;= 1.2, &quot;Moderate&quot;, ifelse(abs(`Effect Size (Cohen)`) &lt;= 2, &quot;Large&quot;, ifelse(abs(`Effect Size (Cohen)`) &lt;= 4, &quot;Very Large&quot;, &quot;Nearly Perfect&quot;)))))) %&gt;%
  mutate(`Cohen&#39;s Interpretation` = ifelse(abs(`Effect Size (Cohen)`) &lt; 0.5, &quot;Small&quot;, ifelse(abs(`Effect Size (Cohen)`) &lt; 0.8, &quot;Medium&quot;, &quot;Large&quot;)))


knitr::kable(effect_interpretation)</code></pre>
<table>
<thead>
<tr class="header">
<th align="left">Season</th>
<th align="right">Effect Size (Cohen)</th>
<th align="left">Hopkins Interpretation</th>
<th align="left">Cohen’s Interpretation</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Preseason - Inseason</td>
<td align="right">0.92</td>
<td align="left">Moderate</td>
<td align="left">Large</td>
</tr>
<tr class="even">
<td align="left">Preseason - Postseason</td>
<td align="right">0.88</td>
<td align="left">Moderate</td>
<td align="left">Large</td>
</tr>
<tr class="odd">
<td align="left">Postseason - Inseason</td>
<td align="right">0.03</td>
<td align="left">Trivial</td>
<td align="left">Small</td>
</tr>
</tbody>
</table>
<p>As noted earlier and worth mentioning again, there are a number of ways to calculate standardized effect sizes and there really is not a universally accepted “right” way. Using residual SD and the square root of the sum of variance components from a mixed model for standardization are not traditional Cohen’s d statistics, so they should be referred to as standardized effect sizes. What we noticed from this dataset is that using the residual variance and all variance components in the denominator produced similar effect statistics becuase the within-athlete variability was quite small compared to the between-athlete variability. Both model effect statistic methods produced similar results than what we got by calculating cohen’s d directly from the data, again due to lower within-athlete variability. Although the effect statistics were similar in this simulated dataset, this will not always be the case, particularly when more complex models are used (e.g., multiple random intercepts, random slopes, or nested models) The important thing is to detail which method you are using and make sure to label it appropriately.</p>
</div>
</div>
</div>
